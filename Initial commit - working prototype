import json
import os
from enum import Enum
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import sys
import re
from collections import Counter

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫)
RUSSIAN_STOPWORDS = {
    "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–∫", "—É", "–æ", "–æ–±", "–æ—Ç", "–¥–æ", "–∑–∞",
    "–∏–∑", "–∏–ª–∏", "–Ω–æ", "–∞", "–∂–µ", "–±—ã", "–ª–∏", "–∂–µ", "–≤–æ—Ç", "—á—Ç–æ", "—ç—Ç–æ",
    "–∫–∞–∫", "—Ç–∞–∫", "–¥–ª—è", "—Ç–æ", "—Å–æ", "–ø—Ä–∏", "–Ω–µ", "–Ω–∏", "–¥–∞", "–Ω–µ—Ç",
    "–∏–∑-–∑–∞", "–ø–æ–¥", "–Ω–∞–¥", "–ø–µ—Ä–µ–¥", "–ø–æ—Å–ª–µ", "—á–µ—Ä–µ–∑", "–º–µ–∂–¥—É"
}


class UserRole(Enum):
    JUDGE = "—Å—É–¥—å—è"
    LAWYER = "–∞–¥–≤–æ–∫–∞—Ç"
    PROSECUTOR = "–ø—Ä–æ–∫—É—Ä–æ—Ä"


@dataclass
class LegalCase:
    case_id: str
    description: str
    case_type: str
    parties: List[str]
    claims: List[str]
    documents: List[str]
    created_at: str
    user_role: str
    status: str = "–≤ —Ä–∞–±–æ—Ç–µ"


@dataclass
class AnalysisResult:
    similar_cases: List[Dict]
    legal_norms: List[str]
    risk_score: float
    recommendations: List[str]
    contradictions: List[str]
    predicted_outcome: str = ""
    role_specific_insights: List[str] = None
    nlp_analysis: Dict[str, Any] = None

    def __post_init__(self):
        if self.role_specific_insights is None:
            self.role_specific_insights = []
        if self.nlp_analysis is None:
            self.nlp_analysis = {}


@dataclass
class PersonalizedReport:
    role: UserRole
    generated_at: str
    case_id: str
    case_description: str
    key_insights: List[str]
    actions: List[str]
    warnings: List[str]
    documents_summary: List[str]
    raw_analysis: AnalysisResult
    nlp_summary: Dict[str, Any] = None

    def __post_init__(self):
        if self.nlp_summary is None:
            self.nlp_summary = {}


class NLPAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ NLP-–∞–Ω–∞–ª–∏–∑–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–±–µ–∑ NLTK)"""

    @staticmethod
    def simple_tokenize(text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–µ–∑ NLTK"""
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        return [word for word in text_clean.split() if word.strip()]

    @staticmethod
    def analyze_text(text: str) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ NLTK"""
        if not text or not isinstance(text, str):
            return {"error": "–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —Å—Ç—Ä–æ–∫–∞"}

        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            words = NLPAnalyzer.simple_tokenize(text)
            sentences = [s.strip() for s in text.split('.') if s.strip()]

            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
            filtered_words = [word for word in words if word not in RUSSIAN_STOPWORDS]

            # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            word_freq = Counter(filtered_words)
            top_keywords = word_freq.most_common(10)

            # –ü–æ–∏—Å–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            legal_terms = NLPAnalyzer.extract_legal_terms(text)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—É–º–º –∏ –¥–∞—Ç
            monetary_amounts = NLPAnalyzer.extract_monetary_amounts(text)
            dates = NLPAnalyzer.extract_dates(text)

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏
            sentiment = NLPAnalyzer.analyze_sentiment(text)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∑–∞–∫–æ–Ω—ã
            law_references = NLPAnalyzer.extract_law_references(text)

            # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
            complexity_score = NLPAnalyzer.calculate_complexity(text, sentences)

            return {
                "word_count": len(words),
                "sentence_count": len(sentences),
                "filtered_word_count": len(filtered_words),
                "top_keywords": [{"word": w, "count": c} for w, c in top_keywords],
                "legal_terms_found": legal_terms,
                "monetary_amounts": monetary_amounts,
                "dates_found": dates,
                "sentiment": sentiment,
                "law_references": law_references,
                "complexity_score": complexity_score,
                "avg_sentence_length": len(words) / len(sentences) if sentences else 0,
                "unique_words": len(set(filtered_words))
            }

        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ NLP-–∞–Ω–∞–ª–∏–∑–∞: {str(e)}"}

    @staticmethod
    def extract_legal_terms(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        legal_keywords = [
            # –ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            "–∏—Å–∫", "–∏—Å–∫–æ–≤–æ–µ", "–∏—Å—Ç–µ—Ü", "–æ—Ç–≤–µ—Ç—á–∏–∫", "—Å—É–¥", "—Å—É–¥—å—è",
            "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ", "—Å–≤–∏–¥–µ—Ç–µ–ª—å", "—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞", "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ",
            "—Ö–æ–¥–∞—Ç–∞–π—Å—Ç–≤–æ", "–∂–∞–ª–æ–±–∞", "–∞–ø–µ–ª–ª—è—Ü–∏—è", "–∫–∞—Å—Å–∞—Ü–∏—è",
            "–ø—Ä–∏–≥–æ–≤–æ—Ä", "—Ä–µ—à–µ–Ω–∏–µ", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ",

            # –ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            "–¥–æ–≥–æ–≤–æ—Ä", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ",
            "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–≤–∏–Ω–∞", "—É–º—ã—Å–µ–ª", "–Ω–µ–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å",
            "—É—â–µ—Ä–±", "–≤—Ä–µ–¥", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ",
            "–Ω–µ—É—Å—Ç–æ–π–∫–∞", "–ø–µ–Ω—è", "—à—Ç—Ä–∞—Ñ", "–∑–∞–¥–∞—Ç–æ–∫",
            "–∑–∞–ª–æ–≥", "–∏–ø–æ—Ç–µ–∫–∞", "–∑–∞–≤–µ—â–∞–Ω–∏–µ", "–Ω–∞—Å–ª–µ–¥—Å—Ç–≤–æ",

            # –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã
            "–≥–∫ —Ä—Ñ", "—É–∫ —Ä—Ñ", "–≥–ø–∫ —Ä—Ñ", "—É–ø–∫ —Ä—Ñ", "–Ω–∫ —Ä—Ñ",
            "—Ç–∫ —Ä—Ñ", "—Å–∫ —Ä—Ñ", "–∑–ø–ø", "–∫–æ–∞–ø", "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω"
        ]

        found_terms = []
        text_lower = text.lower()

        for term in legal_keywords:
            if term in text_lower:
                found_terms.append(term)

        return list(set(found_terms))[:15]

    @staticmethod
    def extract_monetary_amounts(text: str) -> List[Dict[str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—É–º–º –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        patterns = [
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å—É–º–º –≤ —Ä—É–±–ª—è—Ö
            (r'(\d+(?:[\s\d]*\d+)?)\s*(—Ç—ã—Å|—Ç—ã—Å—è—á|—Ç\.?\s*—Ä\.?)\b',
             lambda m: f"{m.group(1).replace(' ', '')} —Ç—ã—Å. —Ä—É–±."),
            (r'(\d+(?:[\s\d]*\d+)?)\s*(–º–ª–Ω|–º–∏–ª–ª–∏–æ–Ω|–º–ª–Ω\.?)\s*—Ä(—É–±)?\.?',
             lambda m: f"{m.group(1).replace(' ', '')} –º–ª–Ω —Ä—É–±."),
            (r'(\d+(?:[\s\d]*\d+)?)\s*—Ä(—É–±)?(–ª–µ–π)?\.?',
             lambda m: f"{m.group(1).replace(' ', '')} —Ä—É–±."),
            (r'\$?\s*(\d+(?:[\s\d]*\d+)?)\s*(–¥–æ–ª–ª–∞—Ä|–µ–≤—Ä–æ|‚Ç¨|\$)',
             lambda m: f"{m.group(1).replace(' ', '')} {m.group(2)}"),
        ]

        amounts = []
        for pattern, formatter in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    amount_str = formatter(match)
                    context_start = max(0, match.start() - 50)
                    context_end = min(len(text), match.end() + 50)
                    context = text[context_start:context_end]

                    amounts.append({
                        "amount": amount_str,
                        "context": f"...{context}...",
                        "position": (match.start(), match.end())
                    })
                except:
                    continue

        return amounts[:10]

    @staticmethod
    def extract_dates(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        date_patterns = [
            r'\d{1,2}\.\d{1,2}\.\d{4}',  # DD.MM.YYYY
            r'\d{1,2}\.\d{1,2}\.\d{2}',  # DD.MM.YY
            r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
        ]

        dates = []
        for pattern in date_patterns:
            found_dates = re.findall(pattern, text, re.IGNORECASE)
            dates.extend(found_dates)

        return list(set(dates))[:10]

    @staticmethod
    def extract_law_references(text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∑–∞–∫–æ–Ω—ã –∏ —Å—Ç–∞—Ç—å–∏"""
        patterns = [
            r'(—Å—Ç\.|—Å—Ç–∞—Ç—å—è|–ø\.|–ø—É–Ω–∫—Ç|—á\.|—á–∞—Å—Ç—å)\s*(\d+(?:\s*[–∞-—è]?)?)\s*(–ì–ö|–£–ö|–ù–ö|–¢–ö|–ñ–ö|–ó–ö|–í–ö|–ë–ö|–ê–ü–ö|–ì–ü–ö|–£–ü–ö|–ö–æ–ê–ü|–°–ö)\s*(–†–§)?',
            r'(–§–µ–¥–µ—Ä–∞–ª—å–Ω(—ã–π|–æ–≥–æ)|–§–ó)\s*(–∑–∞–∫–æ–Ω|–∑–∞–∫–æ–Ω–∞)\s*(‚Ññ?\s*\d+[-\d]*)',
            r'–ö–æ–¥–µ–∫—Å(?:–∞|–æ–º)?\s*(–†–§)?\s*(–æ–±|–æ|–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω|–≥—Ä–∞–∂–¥–∞–Ω—Å–∫|—É–≥–æ–ª–æ–≤–Ω|—Ç—Ä—É–¥–æ–≤|—Å–µ–º–µ–π–Ω|–Ω–∞–ª–æ–≥–æ–≤|–∑–µ–º–µ–ª—å–Ω|–≤–æ–¥–Ω|–ª–µ—Å–Ω|–±—é–¥–∂–µ—Ç–Ω|–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω)',
        ]

        references = []
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                references.append(match.group(0))

        return list(set(references))[:10]

    @staticmethod
    def analyze_sentiment(text: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ NLTK"""
        positive_words = {
            "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω", "–≤—ã–∏–≥—Ä–∞–ª", "–ø—Ä–∞–≤", "—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤", "–∑–∞–∫–æ–Ω–Ω",
            "–æ–±–æ—Å–Ω–æ–≤–∞–Ω", "–¥–æ–∫–∞–∑–∞–Ω", "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω", "–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω", "–∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞–Ω"
        }

        negative_words = {
            "–æ—Ç–∫–∞–∑–∞—Ç—å", "–æ—Ç–∫–∞–∑", "–Ω–∞—Ä—É—à–µ–Ω", "–Ω–µ–ø—Ä–∞–≤–æ–º–µ—Ä–Ω", "–Ω–µ–∑–∞–∫–æ–Ω–Ω",
            "—É—â–µ—Ä–±", "–≤—Ä–µ–¥", "–ø–æ—Ç–µ—Ä—è", "—É–±—ã—Ç–æ–∫", "—à—Ç—Ä–∞—Ñ", "–ø–µ–Ω—è",
            "–Ω–µ—É—Å—Ç–æ–π–∫–∞", "–≤–∏–Ω–æ–≤–µ–Ω", "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–Ω–∞–∫–∞–∑–∞–Ω–∏–µ"
        }

        neutral_words = {
            "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ", "–∑–∞—è–≤–ª–µ–Ω–∏–µ", "–∏—Å–∫", "–¥–æ–∫—É–º–µ–Ω—Ç", "—Å—É–¥",
            "—Ä–µ—à–µ–Ω–∏–µ", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–∑–∞–∫–æ–Ω", "—Å—Ç–∞—Ç—å—è", "–ø—É–Ω–∫—Ç"
        }

        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words = NLPAnalyzer.simple_tokenize(text)

        # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        positive_count = sum(1 for w in words if any(pw in w for pw in positive_words))
        negative_count = sum(1 for w in words if any(nw in w for nw in negative_words))
        neutral_count = sum(1 for w in words if any(nw in w for nw in neutral_words))

        total_scored = positive_count + negative_count + neutral_count
        if total_scored == 0:
            return {"sentiment": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π", "score": 0.5, "confidence": 0.0}

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫—É
        sentiment_score = (positive_count - negative_count) / total_scored
        confidence = total_scored / len(words) if words else 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if sentiment_score > 0.2:
            sentiment = "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π"
        elif sentiment_score < -0.2:
            sentiment = "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π"
        else:
            sentiment = "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π"

        return {
            "sentiment": sentiment,
            "score": round((sentiment_score + 1) / 2, 2),
            "confidence": round(confidence, 2),
            "positive_words": positive_count,
            "negative_words": negative_count,
            "neutral_words": neutral_count
        }

    @staticmethod
    def calculate_complexity(text: str, sentences: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ NLTK"""
        if not sentences:
            return 0.0

        words = NLPAnalyzer.simple_tokenize(text)

        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        avg_sentence_length = len(words) / len(sentences)

        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
        avg_word_length = sum(len(w) for w in words) / len(words) if words else 0

        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ª–µ–∫—Å–∏–∫–∏
        unique_word_ratio = len(set(words)) / len(words) if words else 0

        # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        legal_terms = NLPAnalyzer.extract_legal_terms(text)
        legal_term_ratio = len(legal_terms) / len(words) if words else 0

        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity = (
                min(avg_sentence_length / 20, 1.0) * 0.3 +
                min(avg_word_length / 8, 1.0) * 0.2 +
                unique_word_ratio * 0.3 +
                min(legal_term_ratio * 10, 1.0) * 0.2
        )

        return round(complexity, 2)

    @staticmethod
    def compare_texts(text1: str, text2: str) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–µ–∑ NLTK"""
        analysis1 = NLPAnalyzer.analyze_text(text1)
        analysis2 = NLPAnalyzer.analyze_text(text2)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords1 = set([item["word"] for item in analysis1.get("top_keywords", [])])
        keywords2 = set([item["word"] for item in analysis2.get("top_keywords", [])])

        common_keywords = keywords1.intersection(keywords2)

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        sentiment_diff = abs(analysis1.get("sentiment", {}).get("score", 0.5) -
                             analysis2.get("sentiment", {}).get("score", 0.5))

        return {
            "similarity_keywords": len(common_keywords) / max(len(keywords1.union(keywords2)), 1),
            "common_keywords": list(common_keywords)[:10],
            "sentiment_difference": round(sentiment_diff, 2),
            "complexity_comparison": {
                "text1": analysis1.get("complexity_score", 0),
                "text2": analysis2.get("complexity_score", 0)
            }
        }


class DataManager:
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.cases_file = os.path.join(data_dir, "cases.json")
        self.reports_dir = os.path.join(data_dir, "reports")
        self._ensure_directories()

    def _ensure_directories(self):
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)
        if not os.path.exists(self.cases_file):
            with open(self.cases_file, 'w', encoding='utf-8') as f:
                json.dump([], f, ensure_ascii=False, indent=2)

    def save_case(self, case: LegalCase) -> bool:
        try:
            cases = self.load_cases()
            cases.append(asdict(case))
            with open(self.cases_file, 'w', encoding='utf-8') as f:
                json.dump(cases, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–µ–ª–∞: {e}")
            return False

    def load_cases(self) -> List[Dict]:
        try:
            if not os.path.exists(self.cases_file):
                return []
            with open(self.cases_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except (json.JSONDecodeError, IOError) as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–µ–ª: {e}")
            return []

    def save_report(self, report: PersonalizedReport) -> Optional[str]:
        try:
            report_dict = asdict(report)
            report_dict['role'] = report.role.value
            filename = f"report_{report.case_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join(self.reports_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report_dict, f, ensure_ascii=False, indent=2)
            return filepath
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞: {e}")
            return None


class RoleSpecificQuestionnaire:
    @staticmethod
    def get_questions_for_role(role: UserRole) -> List[Tuple[str, str]]:
        questions_map = {
            UserRole.JUDGE: [
                ("qualification", "–ò–º–µ—é—Ç—Å—è –ª–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è/—Å–ø–æ—Ä–∞?"),
                ("evidence", "–ï—Å—Ç—å –ª–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞—Ö —Å—Ç–æ—Ä–æ–Ω?"),
                ("expertise", "–¢—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å—É–¥–µ–±–Ω–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã?"),
                ("norms", "–ö–∞–∫–∏–µ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞ –≤—ã–∑—ã–≤–∞—é—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ —Ç–æ–ª–∫–æ–≤–∞–Ω–∏–∏?"),
                ("similarity", "–ò–∑–≤–µ—Å—Ç–Ω—ã –ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –¥–µ–ª–∞ –≤ –ø—Ä–∞–∫—Ç–∏–∫–µ —ç—Ç–æ–≥–æ —Å—É–¥–∞?")
            ],
            UserRole.LAWYER: [
                ("strategy", "–ö–∞–∫–æ–≤–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∞—è —Ü–µ–ª—å –ø–æ –¥–µ–ª—É (–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è, –ø—Ä–∏–∑–Ω–∞–Ω–∏–µ –ø—Ä–∞–≤–∞ –∏ —Ç.–¥.)?"),
                ("weaknesses", "–ö–∞–∫–∏–µ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –≤ –ø–æ–∑–∏—Ü–∏–∏ –≤–∞—à–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞?"),
                ("opponent", "–ö–∞–∫–∏–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —É –ø—Ä–æ—Ç–∏–≤–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã?"),
                ("petitions", "–ö–∞–∫–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ —Ö–æ–¥–∞—Ç–∞–π—Å—Ç–≤–∞ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –∑–∞—è–≤–∏—Ç—å?"),
                ("evidence_needed", "–ö–∞–∫–∏–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è?")
            ],
            UserRole.PROSECUTOR: [
                ("evidence_completeness", "–ü–æ–ª–Ω–æ—Ç–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø–æ –¥–µ–ª—É?"),
                ("qualification_match", "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞—Ç—å–µ –£–ö –†–§?"),
                ("circumstances", "–£—á—Ç–µ–Ω—ã –ª–∏ –≤—Å–µ —Å–º—è–≥—á–∞—é—â–∏–µ/–æ—Ç—è–≥—á–∞—é—â–∏–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞?"),
                ("investigation", "–¢—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ?"),
                ("witnesses", "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –ø–æ–∫–∞–∑–∞–Ω–∏–π —Å–≤–∏–¥–µ—Ç–µ–ª–µ–π?")
            ]
        }
        return questions_map.get(role, [])

    @staticmethod
    def get_document_types_for_role(role: UserRole) -> List[str]:
        doc_map = {
            UserRole.JUDGE: ["–ò—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ", "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Å—Ç–æ—Ä–æ–Ω", "–•–æ–¥–∞—Ç–∞–π—Å—Ç–≤–∞",
                             "–ó–∞–∫–ª—é—á–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "–ü—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ—à–µ–Ω–∏—è"],
            UserRole.LAWYER: ["–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", "–ü–æ–∑–∏—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞", "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞",
                              "–í–æ–∑—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –∏—Å–∫", "–†–∞—Å—á–µ—Ç—ã —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"],
            UserRole.PROSECUTOR: ["–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –ø—Ä–æ–≤–µ—Ä–∫–∏", "–û–±–≤–∏–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ",
                                  "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞", "–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "–ü—Ä–æ—Ç–æ–∫–æ–ª—ã"]
        }
        return doc_map.get(role, [])


class LegalAnalyticsCore:
    def __init__(self):
        self.nlp_analyzer = NLPAnalyzer()

        # –ë–∞–∑–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–µ–ª –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.case_examples_db = [
            {
                "id": "–ê40-123456/2023",
                "description": "–í–∑—ã—Å–∫–∞–Ω–∏–µ –¥–æ–ª–≥–∞ –ø–æ –¥–æ–≥–æ–≤–æ—Ä—É –∑–∞–π–º–∞ –≤ —Ä–∞–∑–º–µ—Ä–µ 500000 —Ä—É–±–ª–µ–π",
                "category": "–¥–æ–ª–≥",
                "outcome": "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–æ",
                "court": "–ê–° –≥. –ú–æ—Å–∫–≤—ã",
                "keywords": ["–¥–æ–ª–≥", "–∑–∞–µ–º", "–¥–æ–≥–æ–≤–æ—Ä", "–≤–∑—ã—Å–∫–∞–Ω–∏–µ", "—Ä—É–±–ª—å"]
            },
            {
                "id": "–ê41-789012/2022",
                "description": "–¢—Ä—É–¥–æ–≤–æ–π —Å–ø–æ—Ä –æ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ–º —É–≤–æ–ª—å–Ω–µ–Ω–∏–∏ –∏ –≤–∑—ã—Å–∫–∞–Ω–∏–∏ –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã",
                "category": "—Ç—Ä—É–¥–æ–≤–æ–π",
                "outcome": "—á–∞—Å—Ç–∏—á–Ω–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–æ",
                "court": "–ê–° –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
                "keywords": ["—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ", "–∑–∞—Ä–ø–ª–∞—Ç–∞", "—Ç—Ä—É–¥–æ–≤–æ–π", "–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ"]
            },
            {
                "id": "1-123/2023",
                "description": "–£–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ –ø–æ —Å—Ç–∞—Ç—å–µ 158 –£–ö –†–§ (–∫—Ä–∞–∂–∞)",
                "category": "—É–≥–æ–ª–æ–≤–Ω—ã–π",
                "outcome": "–æ–±–≤–∏–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–≥–æ–≤–æ—Ä",
                "court": "–ú–æ—Å–≥–æ—Ä—Å—É–¥",
                "keywords": ["–∫—Ä–∞–∂–∞", "—É–≥–æ–ª–æ–≤–Ω—ã–π", "–ø—Ä–∏–≥–æ–≤–æ—Ä", "—Å—Ç–∞—Ç—å—è 158"]
            },
            {
                "id": "33–∞-4567/2022",
                "description": "–ó–∞—â–∏—Ç–∞ –ø—Ä–∞–≤ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è, –≤–æ–∑–≤—Ä–∞—Ç –¥–µ–Ω–µ–≥ –∑–∞ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä",
                "category": "–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å",
                "outcome": "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–æ",
                "court": "–í–° –†–§",
                "keywords": ["–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å", "—Ç–æ–≤–∞—Ä", "–≤–æ–∑–≤—Ä–∞—Ç", "–∫–∞—á–µ—Å—Ç–≤–æ"]
            }
        ]

    def analyze_case(self, case: LegalCase, role_answers: Dict[str, str] = None) -> AnalysisResult:
        """–ê–Ω–∞–ª–∏–∑ –¥–µ–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Å—Ç–æ–≥–æ NLP"""

        # NLP-–∞–Ω–∞–ª–∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –¥–µ–ª–∞ (–±–µ–∑ NLTK)
        nlp_analysis = self.nlp_analyzer.analyze_text(case.description)

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–µ–ª
        similar_cases = self.find_similar_cases(case.description)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        case_type = case.case_type.lower()
        legal_norms = []
        predicted_outcome = "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω"

        if "–∞—Ä–±–∏—Ç—Ä–∞–∂" in case_type or "–≥—Ä–∞–∂–¥–∞–Ω" in case_type:
            legal_norms = ["—Å—Ç. 15 –ì–ö –†–§", "—Å—Ç. 395 –ì–ö –†–§", "—Å—Ç. 10 –ì–ö –†–§"]
            predicted_outcome = self.predict_outcome(nlp_analysis, "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π")
        elif "—É–≥–æ–ª–æ–≤" in case_type:
            legal_norms = ["—Å—Ç. 158 –£–ö –†–§", "—Å—Ç. 105 –£–ö –†–§", "—Å—Ç. 111 –£–ö –†–§"]
            predicted_outcome = self.predict_outcome(nlp_analysis, "—É–≥–æ–ª–æ–≤–Ω—ã–π")
        else:
            legal_norms = ["—Å—Ç. 46 –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏ –†–§"]
            predicted_outcome = self.predict_outcome(nlp_analysis, "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π")

        # –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        risk_score = self.calculate_risk_score(nlp_analysis)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations = self.generate_recommendations(case.user_role, nlp_analysis, role_answers)

        # –ü–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
        contradictions = self.find_contradictions(case, nlp_analysis)

        # –†–æ–ª–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        role_insights = self.generate_role_insights(case.user_role, nlp_analysis, role_answers)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –Ω–æ—Ä–º –ø—Ä–∞–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        law_references = nlp_analysis.get("law_references", [])
        if law_references:
            legal_norms.extend(law_references[:3])

        return AnalysisResult(
            similar_cases=similar_cases,
            legal_norms=list(set(legal_norms))[:5],
            risk_score=risk_score,
            recommendations=recommendations,
            contradictions=contradictions,
            predicted_outcome=predicted_outcome,
            role_specific_insights=role_insights,
            nlp_analysis=nlp_analysis
        )

    def find_similar_cases(self, query: str) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–µ–ª –±–µ–∑ —Å–ª–æ–∂–Ω–æ–≥–æ NLP"""
        query_lower = query.lower()
        query_words = set(self.nlp_analyzer.simple_tokenize(query))

        results = []
        for case in self.case_examples_db:
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            case_keywords = set(case.get("keywords", []))
            keyword_overlap = len(query_words.intersection(case_keywords))

            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            case_desc_words = set(self.nlp_analyzer.simple_tokenize(case["description"]))
            desc_keyword_overlap = len(query_words.intersection(case_desc_words))

            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            direct_match = any(keyword in query_lower for keyword in case["keywords"])

            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
            total_similarity = (keyword_overlap + desc_keyword_overlap)
            if direct_match:
                total_similarity += 2

            similarity_score = min(total_similarity / 10, 1.0)

            if similarity_score > 0.3:
                results.append({
                    "id": case["id"],
                    "description": case["description"],
                    "category": case["category"],
                    "outcome": case["outcome"],
                    "court": case["court"],
                    "similarity": round(similarity_score, 2)
                })

        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:3]

    def predict_outcome(self, nlp_analysis: Dict[str, Any], case_type: str) -> str:
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        sentiment = nlp_analysis.get("sentiment", {})
        sentiment_score = sentiment.get("score", 0.5)

        if case_type == "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π":
            if sentiment_score > 0.7:
                return "–ø–æ–ª–Ω–æ–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ –∏—Å–∫–∞"
            elif sentiment_score > 0.5:
                return "—á–∞—Å—Ç–∏—á–Ω–æ–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ"
            elif sentiment_score > 0.3:
                return "–æ—Ç–∫–∞–∑ –≤ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–∏"
            else:
                return "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞"

        elif case_type == "—É–≥–æ–ª–æ–≤–Ω—ã–π":
            if sentiment_score > 0.6:
                return "–æ–±–≤–∏–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–≥–æ–≤–æ—Ä"
            elif sentiment_score > 0.4:
                return "–æ–ø—Ä–∞–≤–¥–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–≥–æ–≤–æ—Ä"
            else:
                return "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –¥–µ–ª–∞"

        else:
            if sentiment_score > 0.6:
                return "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"
            else:
                return "–æ—Ç–∫–∞–∑ –≤ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–∏"

    def calculate_risk_score(self, nlp_analysis: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞"""
        sentiment = nlp_analysis.get("sentiment", {})
        sentiment_score = sentiment.get("score", 0.5)
        complexity = nlp_analysis.get("complexity_score", 0.5)

        risk_from_sentiment = 1.0 - sentiment_score
        risk_from_complexity = complexity

        total_risk = (risk_from_sentiment * 0.6 + risk_from_complexity * 0.4)
        return round(min(max(total_risk, 0.0), 1.0), 2)

    def generate_recommendations(self, role: str, nlp_analysis: Dict[str, Any],
                                 role_answers: Dict[str, str] = None) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []

        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        complexity = nlp_analysis.get("complexity_score", 0.5)
        if complexity > 0.7:
            recommendations.append("–î–µ–ª–æ –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ç—â–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        # –†–æ–ª–µ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if role == "—Å—É–¥—å—è":
            recommendations.extend([
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Å–∫–æ–≤—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å—Ç. 131 –ì–ü–ö –†–§",
                "–£—á–µ—Å—Ç—å –≤—Å–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã –∏ –¥–∞—Ç—ã",
                f"–°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {complexity} - –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —É—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞"
            ])
        elif role == "–∞–¥–≤–æ–∫–∞—Ç":
            recommendations.extend([
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ö–æ–¥–∞—Ç–∞–π—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π",
                "–£—Å–∏–ª–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ —Å–ª–∞–±—ã–º –º–µ—Å—Ç–∞–º"
            ])
        elif role == "–ø—Ä–æ–∫—É—Ä–æ—Ä":
            recommendations.extend([
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–ø—É—Å—Ç–∏–º–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø–æ —Å—Ç. 75 –£–ü–ö –†–§",
                "–°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤—ã–≤–æ–¥—ã —Å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º–∏ –¥–µ–ª–∞–º–∏"
            ])

        return recommendations

    def find_contradictions(self, case: LegalCase, nlp_analysis: Dict[str, Any]) -> List[str]:
        """–ü–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –¥–µ–ª–µ"""
        contradictions = []

        if len(case.documents) < 2:
            contradictions.append(f"–ú–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({len(case.documents)}). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ.")

        monetary_amounts = nlp_analysis.get("monetary_amounts", [])
        if len(monetary_amounts) == 0:
            contradictions.append("–í —Ç–µ–∫—Å—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã")
        elif len(monetary_amounts) > 3:
            contradictions.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –≤ –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—É–º–º–∞—Ö - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞")

        dates = nlp_analysis.get("dates_found", [])
        if len(dates) == 0:
            contradictions.append("–í —Ç–µ–∫—Å—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞—Ç—ã")

        return contradictions

    def generate_role_insights(self, role: str, nlp_analysis: Dict[str, Any],
                               role_answers: Dict[str, str] = None) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–æ–ª–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤"""
        insights = []

        word_count = nlp_analysis.get("word_count", 0)
        if word_count < 100:
            insights.append("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫—Ä–∞—Ç–∫–∏–π - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
        elif word_count > 1000:
            insights.append("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –æ–±—ä–µ–º–Ω—ã–π - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤")

        if role == "—Å—É–¥—å—è":
            sentiment = nlp_analysis.get("sentiment", {})
            if sentiment.get("sentiment") == "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π":
                insights.append("–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –≤–æ–∑–º–æ–∂–Ω—É—é –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö")
        elif role == "–∞–¥–≤–æ–∫–∞—Ç":
            legal_terms = nlp_analysis.get("legal_terms_found", [])
            if len(legal_terms) > 5:
                insights.append(
                    "–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ - —Ö–æ—Ä–æ—à–∞—è –±–∞–∑–∞ –¥–ª—è –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏")

        return insights


class ReportPersonalizer:
    def __init__(self):
        self.nlp_analyzer = NLPAnalyzer()

    def personalize(self, role: UserRole, case: LegalCase, analysis: AnalysisResult) -> PersonalizedReport:
        """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞"""

        # –ë–∞–∑–æ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        base_insights = [
            f"–ù–∞–π–¥–µ–Ω–æ {len(analysis.similar_cases)} –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –¥–µ–ª",
            f"–ü—Ä–∏–º–µ–Ω–∏–º—ã–µ –Ω–æ—Ä–º—ã: {', '.join(analysis.legal_norms[:3])}",
            f"–û—Ü–µ–Ω–∫–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤: {analysis.risk_score:.0%}",
            f"–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–π –∏—Å—Ö–æ–¥: {analysis.predicted_outcome}"
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º NLP-–∏–Ω—Å–∞–π—Ç—ã
        nlp_data = analysis.nlp_analysis or {}
        if nlp_data and "error" not in nlp_data:
            nlp_insights = [
                f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {nlp_data.get('sentiment', {}).get('sentiment', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}",
                f"–°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {nlp_data.get('complexity_score', 0)} –∏–∑ 1.0",
                f"–ù–∞–π–¥–µ–Ω–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(nlp_data.get('legal_terms_found', []))}"
            ]
            base_insights.extend(nlp_insights)

        if analysis.role_specific_insights:
            base_insights.extend(analysis.role_specific_insights)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏—è
        actions = self._generate_actions(role, analysis)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        warnings = self._generate_warnings(role, analysis)

        # –°—É–º–º–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents_summary = self._summarize_documents(case, role)

        # –°–æ–∑–¥–∞–µ–º NLP-—Ä–µ–∑—é–º–µ
        nlp_summary = self._create_nlp_summary(nlp_data)

        return PersonalizedReport(
            role=role,
            generated_at=datetime.now().isoformat(),
            case_id=case.case_id,
            case_description=case.description[:100] + ("..." if len(case.description) > 100 else ""),
            key_insights=base_insights,
            actions=actions,
            warnings=warnings,
            documents_summary=documents_summary,
            raw_analysis=analysis,
            nlp_summary=nlp_summary
        )

    def _generate_actions(self, role: UserRole, analysis: AnalysisResult) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Ä–æ–ª–∏"""
        actions = []

        if role == UserRole.JUDGE:
            actions = [
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –¥–µ–ª–∞",
                f"–°—Ä–∞–≤–Ω–∏—Ç—å —Å –ø—Ä–∞–∫—Ç–∏–∫–æ–π: {analysis.similar_cases[0]['id'] if analysis.similar_cases else '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'}",
                "–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—É—é —Å—É–º–º—É –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–æ–≥–æ–≤"
            ]

        elif role == UserRole.LAWYER:
            actions = [
                f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ –¥–µ–ª–æ {analysis.similar_cases[0]['id'] if analysis.similar_cases else 'N/A'}",
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ö–æ–¥–∞—Ç–∞–π—Å—Ç–≤–æ –æ–± —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–µ",
                "–°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø–∏—Å—å–º–µ–Ω–Ω—ã–µ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è"
            ]

        elif role == UserRole.PROSECUTOR:
            actions = [
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –¥–ª—è —Å–æ—Å—Ç–∞–≤–∞",
                "–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–æ–µ–∫—Ç –æ–±–≤–∏–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è",
                "–°–≤–µ—Ä–∏—Ç—å —Å—Ä–æ–∫–∏ –¥–∞–≤–Ω–æ—Å—Ç–∏"
            ]

        return actions

    def _generate_warnings(self, role: UserRole, analysis: AnalysisResult) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π"""
        warnings = []

        if role == UserRole.JUDGE:
            warnings = [
                "–í–Ω–∏–º–∞–Ω–∏–µ: –≤–æ–∑–º–æ–∂–µ–Ω '—ç—Ñ—Ñ–µ–∫—Ç —è–∫–æ—Ä—è' –æ—Ç –∑–∞—è–≤–ª–µ–Ω–Ω–æ–π —Å—É–º–º—ã –∏—Å–∫–∞",
                f"–ö–æ–Ω—Ç—Ä–æ–ª—å: {len(analysis.contradictions)} –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"
            ]

        elif role == UserRole.LAWYER:
            warnings = [
                f"–°–ª–∞–±—ã–µ –º–µ—Å—Ç–∞: {len(analysis.contradictions)} –ø—É–Ω–∫—Ç–æ–≤ —Ç—Ä–µ–±—É—é—Ç –∑–∞—â–∏—Ç—ã",
                f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞: {100 - analysis.risk_score * 100:.0f}%"
            ]

        elif role == UserRole.PROSECUTOR:
            warnings = [
                f"–†–∏—Å–∫ –æ—Ç–∫–∞–∑–∞ –≤ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–∏: {analysis.risk_score:.0%}",
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–ø—É—Å—Ç–∏–º–æ—Å—Ç—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –ø–æ —Å—Ç. 75 –£–ü–ö –†–§"
            ]

        # NLP-–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if analysis.nlp_analysis and "error" not in analysis.nlp_analysis:
            sentiment = analysis.nlp_analysis.get("sentiment", {})
            if sentiment.get("sentiment") == "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π":
                warnings.append("–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å - –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ")

        return warnings

    def _summarize_documents(self, case: LegalCase, role: UserRole) -> List[str]:
        """–°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        documents_summary = []
        doc_types = RoleSpecificQuestionnaire.get_document_types_for_role(role)

        for i, doc in enumerate(case.documents[:3]):
            doc_type = doc_types[i] if i < len(doc_types) else "–î–æ–∫—É–º–µ–Ω—Ç"
            documents_summary.append(f"{doc_type}: {doc}")

        if len(case.documents) > 3:
            documents_summary.append(f"... –∏ –µ—â—ë {len(case.documents) - 3} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        return documents_summary

    def _create_nlp_summary(self, nlp_data: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—é–º–µ NLP-–∞–Ω–∞–ª–∏–∑–∞"""
        if not nlp_data or "error" in nlp_data:
            return {"status": "NLP-–∞–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω", "error": nlp_data.get("error", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")}

        summary = {
            "—Å—Ç–∞—Ç—É—Å": "—É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω",
            "–æ—Å–Ω–æ–≤–Ω—ã–µ_–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏": {
                "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å–ª–æ–≤": nlp_data.get("word_count", 0),
                "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π": nlp_data.get("sentence_count", 0),
                "—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ_—Å–ª–æ–≤–∞": nlp_data.get("unique_words", 0)
            },
            "—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å": nlp_data.get("sentiment", {}),
            "—Å–ª–æ–∂–Ω–æ—Å—Ç—å": nlp_data.get("complexity_score", 0),
            "–Ω–∞–π–¥–µ–Ω–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ": {
                "—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ_—Ç–µ—Ä–º–∏–Ω—ã": len(nlp_data.get("legal_terms_found", [])),
                "–¥–µ–Ω–µ–∂–Ω—ã–µ_—Å—É–º–º—ã": len(nlp_data.get("monetary_amounts", [])),
                "–¥–∞—Ç—ã": len(nlp_data.get("dates_found", [])),
                "—Å—Å—ã–ª–∫–∏_–Ω–∞_–∑–∞–∫–æ–Ω—ã": len(nlp_data.get("law_references", []))
            }
        }

        top_keywords = nlp_data.get("top_keywords", [])
        if top_keywords:
            summary["—Ç–æ–ø_–∫–ª—é—á–µ–≤—ã–µ_—Å–ª–æ–≤–∞"] = [
                {"—Å–ª–æ–≤–æ": kw["word"], "—á–∞—Å—Ç–æ—Ç–∞": kw["count"]}
                for kw in top_keywords[:5]
            ]

        return summary


class JusticeAIAssistant:
    def __init__(self):
        self.data_manager = DataManager()
        self.questionnaire = RoleSpecificQuestionnaire()
        self.core = LegalAnalyticsCore()
        self.personalizer = ReportPersonalizer()
        self.current_role: Optional[UserRole] = None
        self.nlp_analyzer = NLPAnalyzer()

    def select_role(self) -> Optional[UserRole]:
        print("\n" + "=" * 50)
        print("–í–´–ë–ï–†–ò–¢–ï –í–ê–®–£ –†–û–õ–¨")
        print("=" * 50)
        roles = list(UserRole)
        for i, role in enumerate(roles, 1):
            print(f"{i}. {role.value.upper()}")
        print("=" * 50)

        try:
            choice = int(input("–í–∞—à –≤—ã–±–æ—Ä (1-3): ")) - 1
            if 0 <= choice < len(roles):
                self.current_role = roles[choice]
                print(f"\n‚úì –í—ã–±—Ä–∞–Ω–∞ —Ä–æ–ª—å: {self.current_role.value.upper()}")
                return self.current_role
        except (ValueError, IndexError):
            pass
        print("\n‚ö† –†–æ–ª—å –Ω–µ –≤—ã–±—Ä–∞–Ω–∞.")
        return None

    def create_case_for_role(self, role: UserRole) -> Optional[Tuple[LegalCase, Dict]]:
        print("\n" + "=" * 50)
        print(f"–°–û–ó–î–ê–ù–ò–ï –î–ï–õ–ê –î–õ–Ø {role.value.upper()}")
        print("=" * 50)

        try:
            case_id = input("–ù–æ–º–µ—Ä –¥–µ–ª–∞: ").strip() or f"–î–ï–õ–û-{datetime.now().strftime('%Y%m%d-%H%M')}"
            description = input("–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: ").strip() or "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è"
            case_type = input("–¢–∏–ø (–∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π/–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π/—É–≥–æ–ª–æ–≤–Ω—ã–π/–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π): ").strip() or "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π"

            print(f"\nüìé –¢–ò–ü–´ –î–û–ö–£–ú–ï–ù–¢–û–í –î–õ–Ø {role.value.upper()}:")
            doc_types = self.questionnaire.get_document_types_for_role(role)
            for dt in doc_types:
                print(f"  ‚Ä¢ {dt}")

            print("\nüìÅ –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í (–≤–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):")
            docs_input = input("–î–æ–∫—É–º–µ–Ω—Ç—ã: ").strip()
            documents = [d.strip() for d in docs_input.split(",") if d.strip()] or ["–û—Å–Ω–æ–≤–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã"]

            print("\nüìù –û–¢–í–ï–¢–¨–¢–ï –ù–ê –í–û–ü–†–û–°–´ –î–õ–Ø –í–ê–®–ï–ô –†–û–õ–ò:")
            questions = self.questionnaire.get_questions_for_role(role)
            answers = {}
            for key, question in questions:
                answer = input(f"\n{question}\n–û—Ç–≤–µ—Ç: ").strip()
                answers[key] = answer

            print("\nüìÑ –ò–°–ö–û–í–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø/–ü–û–ó–ò–¶–ò–Ø:")
            claims = []
            while True:
                claim = input("  –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ (–ø—É—Å—Ç–æ - –∑–∞–∫–æ–Ω—á–∏—Ç—å): ").strip()
                if not claim:
                    break
                claims.append(claim)

            parties = [f"–°—Ç–æ—Ä–æ–Ω–∞ {i + 1}" for i in range(2)]

            case = LegalCase(
                case_id=case_id,
                description=description,
                case_type=case_type,
                parties=parties,
                claims=claims or ["–ù–µ —É–∫–∞–∑–∞–Ω–æ"],
                documents=documents,
                created_at=datetime.now().isoformat(),
                user_role=role.value
            )

            if self.data_manager.save_case(case):
                print(f"\n‚úÖ –î–µ–ª–æ —Å–æ–∑–¥–∞–Ω–æ! –û—Ç–≤–µ—Ç—ã —É—á—Ç–µ–Ω—ã –≤ –∞–Ω–∞–ª–∏–∑–µ.")
                return case, answers

        except KeyboardInterrupt:
            print("\n\n‚ö† –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ")
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")

        return None, {}

    def analyze_with_context(self, case: LegalCase, answers: Dict[str, str]) -> AnalysisResult:
        return self.core.analyze_case(case, answers)

    def generate_report(self, case: LegalCase, role: UserRole, answers: Dict[str, str] = None) -> Dict[str, Any]:
        analysis = self.analyze_with_context(case, answers or {})
        report = self.personalizer.personalize(role, case, analysis)
        saved_path = self.data_manager.save_report(report)

        return {
            "system": "AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø—Ä–∞–≤–æ—Å—É–¥–∏—è v2.0 —Å NLP (–±–µ–∑ NLTK)",
            "user_role": report.role.value,
            "case_id": report.case_id,
            "case_description": report.case_description,
            "generated_at": report.generated_at,
            "nlp_analysis": report.nlp_summary,
            "report": {
                "key_insights": report.key_insights,
                "recommended_actions": report.actions,
                "warnings": report.warnings,
                "documents": report.documents_summary
            },
            "sources": {
                "similar_cases": report.raw_analysis.similar_cases,
                "legal_norms": report.raw_analysis.legal_norms,
                "risk_score": report.raw_analysis.risk_score,
                "predicted_outcome": report.raw_analysis.predicted_outcome
            },
            "role_context": f"–ê–Ω–∞–ª–∏–∑ —Å —É—á–µ—Ç–æ–º —Ä–æ–ª–∏ {role.value} –∏ NLP",
            "saved_to": saved_path,
            "disclaimer": "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–æ—Å—è—Ç —Å–ø—Ä–∞–≤–æ—á–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä. NLP-–∞–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å –ø–æ–º–æ—â—å—é –ø—Ä–∞–≤–∏–ª –∏ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π."
        }

    def analyze_text_nlp(self, text: str) -> Dict[str, Any]:
        """–û—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è NLP-–∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        return self.nlp_analyzer.analyze_text(text)


class UserInterface:
    def __init__(self):
        self.assistant = JusticeAIAssistant()
        self.current_case = None
        self.current_answers = {}

    def display_main_menu(self):
        while True:
            print("\n" + "=" * 60)
            print("AI-–ê–°–°–ò–°–¢–ï–ù–¢ –î–õ–Ø –ü–†–ê–í–û–°–£–î–ò–Ø (–±–µ–∑ NLTK)")
            print("=" * 60)

            if self.assistant.current_role:
                print(f"–¢–ï–ö–£–©–ê–Ø –†–û–õ–¨: {self.assistant.current_role.value.upper()}")
            else:
                print("–†–û–õ–¨: –Ω–µ –≤—ã–±—Ä–∞–Ω–∞")

            print("\n1. üë§ –í—ã–±—Ä–∞—Ç—å/—Å–º–µ–Ω–∏—Ç—å —Ä–æ–ª—å")
            print("2. üìÅ –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤–æ–µ –¥–µ–ª–æ (—Å —É—á–µ—Ç–æ–º —Ä–æ–ª–∏)")
            print("3. üîç –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—É—â–µ–µ –¥–µ–ª–æ")
            print("4. üìä –ü–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –¥–µ–ª–∞")
            print("5. üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å NLP-–∞–Ω–∞–ª–∏–∑–æ–º")
            print("6. üìÑ –û—Ç—á–µ—Ç—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
            print("7. üîß NLP-–∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
            print("8. üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏")
            print("9. ‚ùå –í—ã—Ö–æ–¥")
            print("=" * 60)

            choice = input("–í—ã–±–æ—Ä (1-9): ").strip()

            if choice == "1":
                self.select_role_menu()
            elif choice == "2":
                self.create_case_menu()
            elif choice == "3":
                self.analyze_case_menu()
            elif choice == "4":
                self.view_cases_menu()
            elif choice == "5":
                self.generate_report_menu()
            elif choice == "6":
                self.view_reports_menu()
            elif choice == "7":
                self.nlp_analysis_menu()
            elif choice == "8":
                self.show_statistics_menu()
            elif choice == "9":
                print("\nüëã –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                sys.exit(0)
            else:
                print("\n‚ö† –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
                input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def select_role_menu(self):
        role = self.assistant.select_role()
        if role:
            print(f"\n‚úì –†–µ–∂–∏–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {role.value.upper()}")
            print("–¢–µ–ø–µ—Ä—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–µ–ª–∞ –±—É–¥—É—Ç –∑–∞–¥–∞–≤–∞—Ç—å—Å—è")
            print("—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –≤–∞—à–µ–π —Ä–æ–ª–∏.")
        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def create_case_menu(self):
        if not self.assistant.current_role:
            print("\n‚ö† –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–æ–ª—å –≤ –º–µ–Ω—é 1!")
            input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
            return

        result = self.assistant.create_case_for_role(self.assistant.current_role)
        if result and result[0]:
            self.current_case, self.current_answers = result
            print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω–æ –¥–µ–ª–æ: {self.current_case.case_id}")
            print("–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç.")
        else:
            print("\n‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–µ–ª–æ.")
        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def analyze_case_menu(self):
        if not self.current_case:
            print("\n‚ö† –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ –¥–µ–ª–æ!")
            input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
            return

        print("\n" + "=" * 60)
        print(f"üîç –ê–ù–ê–õ–ò–ó –î–ï–õ–ê: {self.current_case.case_id}")
        print("=" * 60)

        analysis = self.assistant.analyze_with_context(self.current_case, self.current_answers)

        print(f"\nüìà –û–°–ù–û–í–ù–´–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò:")
        print(f"  ‚Ä¢ –ê–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –¥–µ–ª: {len(analysis.similar_cases)}")
        print(f"  ‚Ä¢ –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–∞: {analysis.risk_score:.0%}")
        print(f"  ‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑ –∏—Å—Ö–æ–¥–∞: {analysis.predicted_outcome}")

        print(f"\n‚öñÔ∏è –ü–†–ò–ú–ï–ù–ò–ú–´–ï –ù–û–†–ú–´:")
        for norm in analysis.legal_norms[:5]:
            print(f"  ‚Ä¢ {norm}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º NLP-–∞–Ω–∞–ª–∏–∑
        if analysis.nlp_analysis and "error" not in analysis.nlp_analysis:
            nlp_data = analysis.nlp_analysis
            print(f"\nüî§ NLP-–ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê (–±–µ–∑ NLTK):")
            print(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {nlp_data.get('word_count', 0)}")
            print(f"  ‚Ä¢ –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {nlp_data.get('sentiment', {}).get('sentiment', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            print(f"  ‚Ä¢ –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {nlp_data.get('complexity_score', 0)}/1.0")

            legal_terms = nlp_data.get('legal_terms_found', [])
            if legal_terms:
                print(f"  ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(legal_terms)}")
                print(f"  ‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã: {', '.join(legal_terms[:3])}")

        if analysis.recommendations:
            print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø {self.assistant.current_role.value.upper()}:")
            for rec in analysis.recommendations[:5]:
                print(f"  ‚Ä¢ {rec}")

        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def generate_report_menu(self):
        if not self.current_case:
            print("\n‚ö† –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–µ–ª–∞!")
            input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
            return

        print("\n" + "=" * 60)
        print("üéØ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–õ–ù–û–ì–û –û–¢–ß–ï–¢–ê –° NLP")
        print("=" * 60)

        report_data = self.assistant.generate_report(
            self.current_case,
            self.assistant.current_role,
            self.current_answers
        )

        print(f"\nüìã –û–¢–ß–ï–¢ –î–õ–Ø: {report_data['user_role'].upper()}")
        print(f"–î–µ–ª–æ: {report_data['case_id']}")
        print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {report_data['case_description']}")
        print("=" * 60)

        print("\nüîç –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´:")
        for insight in report_data['report']['key_insights'][:10]:
            print(f"  ‚Ä¢ {insight}")

        print("\nüéØ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –î–ï–ô–°–¢–í–ò–Ø:")
        for action in report_data['report']['recommended_actions'][:5]:
            print(f"  ‚Ä¢ {action}")

        print("\n‚ö† –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
        for warning in report_data['report']['warnings']:
            print(f"  ‚ö† {warning}")

        # NLP-–∞–Ω–∞–ª–∏–∑
        if report_data.get('nlp_analysis'):
            nlp_info = report_data['nlp_analysis']
            print(f"\nü§ñ NLP-–ê–ù–ê–õ–ò–ó (–±–µ–∑ NLTK):")
            if nlp_info.get('—Å—Ç–∞—Ç—É—Å') == '—É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω':
                metrics = nlp_info.get('–æ—Å–Ω–æ–≤–Ω—ã–µ_–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏', {})
                print(f"  ‚Ä¢ –°–ª–æ–≤: {metrics.get('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_—Å–ª–æ–≤', 0)}")
                print(f"  ‚Ä¢ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {metrics.get('–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π', 0)}")

                sentiment = nlp_info.get('—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', {})
                if sentiment:
                    print(f"  ‚Ä¢ –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment.get('sentiment', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")

                found_data = nlp_info.get('–Ω–∞–π–¥–µ–Ω–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ', {})
                if found_data:
                    print(f"  ‚Ä¢ –î–µ–Ω–µ–∂–Ω—ã—Ö —Å—É–º–º: {found_data.get('–¥–µ–Ω–µ–∂–Ω—ã–µ_—Å—É–º–º—ã', 0)}")
                    print(f"  ‚Ä¢ –î–∞—Ç–∏—Ä–æ–≤–∞–Ω–æ: {found_data.get('–¥–∞—Ç—ã', 0)}")

        saved_msg = f" –≤: {report_data['saved_to']}" if report_data['saved_to'] else ""
        print(f"\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω{saved_msg}")
        print(f"\n{report_data['disclaimer']}")

        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def view_cases_menu(self):
        cases = self.assistant.data_manager.load_cases()
        print("\n" + "=" * 60)
        print(f"üìö –ê–†–•–ò–í –î–ï–õ ({len(cases)})")
        print("=" * 60)

        if not cases:
            print("–ê—Ä—Ö–∏–≤ –ø—É—Å—Ç.")
        else:
            for i, case in enumerate(cases[-10:], 1):
                role_icon = "üë®‚Äç‚öñÔ∏è" if case.get('user_role') == "—Å—É–¥—å—è" else "üë®‚Äçüíº" if case.get(
                    'user_role') == "–∞–¥–≤–æ–∫–∞—Ç" else "üëÆ"
                desc = case['description'][:60] + ("..." if len(case['description']) > 60 else "")
                print(f"\n{i}. {role_icon} {case['case_id']}")
                print(f"   {desc}")
                print(f"   –†–æ–ª—å: {case.get('user_role', '–Ω–µ —É–∫–∞–∑–∞–Ω–∞')}")
                print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(case.get('documents', []))}")

        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def view_reports_menu(self):
        reports_dir = self.assistant.data_manager.reports_dir
        print("\n" + "=" * 60)
        print("üìÑ –°–û–•–†–ê–ù–ï–ù–ù–´–ï –û–¢–ß–ï–¢–´ –° NLP")
        print("=" * 60)

        if not os.path.exists(reports_dir):
            print("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        else:
            try:
                reports = [f for f in os.listdir(reports_dir) if f.endswith('.json')]
                if not reports:
                    print("–û—Ç—á–µ—Ç–æ–≤ –Ω–µ—Ç.")
                else:
                    for r in sorted(reports)[-5:]:
                        filepath = os.path.join(reports_dir, r)
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                report_data = json.load(f)
                                role = report_data.get('role', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                                case_id = report_data.get('case_id', 'N/A')
                                has_nlp = "ü§ñ" if report_data.get('nlp_summary') else ""
                                print(f"\nüìÑ {has_nlp} {r}")
                                print(f"   –î–µ–ª–æ: {case_id}, –†–æ–ª—å: {role}")
                        except:
                            print(f"\nüìÑ {r}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤: {e}")

        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def nlp_analysis_menu(self):
        print("\n" + "=" * 60)
        print("üîß NLP-–ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–õ–¨–ù–û–ì–û –¢–ï–ö–°–¢–ê (–±–µ–∑ NLTK)")
        print("=" * 60)

        print("\n–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–≤–≤–µ–¥–∏—Ç–µ '–≥–æ—Ç–æ–≤–æ' –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ):")
        print("-" * 40)

        lines = []
        while True:
            line = input()
            if line.strip().lower() == '–≥–æ—Ç–æ–≤–æ':
                break
            lines.append(line)

        text = "\n".join(lines)

        if not text.strip():
            print("\n‚ö† –¢–µ–∫—Å—Ç –Ω–µ –≤–≤–µ–¥–µ–Ω.")
            input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")
            return

        print("\n" + "=" * 40)
        print("üîç –í–´–ü–û–õ–ù–Ø–Æ NLP-–ê–ù–ê–õ–ò–ó...")
        print("=" * 40)

        nlp_result = self.assistant.analyze_text_nlp(text)

        if "error" in nlp_result:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {nlp_result['error']}")
        else:
            print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ NLP-–ê–ù–ê–õ–ò–ó–ê (–±–µ–∑ NLTK):")
            print(f"  ‚Ä¢ –°–ª–æ–≤: {nlp_result.get('word_count', 0)}")
            print(f"  ‚Ä¢ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {nlp_result.get('sentence_count', 0)}")

            sentiment = nlp_result.get('sentiment', {})
            if sentiment:
                print(f"  ‚Ä¢ –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment.get('sentiment', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")

            print(f"  ‚Ä¢ –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {nlp_result.get('complexity_score', 0)}/1.0")

            legal_terms = nlp_result.get('legal_terms_found', [])
            if legal_terms:
                print(f"  ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(legal_terms)}")
                print(f"    –ü—Ä–∏–º–µ—Ä—ã: {', '.join(legal_terms[:5])}")

            monetary = nlp_result.get('monetary_amounts', [])
            if monetary:
                print(f"  ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –¥–µ–Ω–µ–∂–Ω—ã—Ö —Å—É–º–º: {len(monetary)}")
                for amount in monetary[:3]:
                    print(f"    - {amount.get('amount', 'N/A')}")

            dates = nlp_result.get('dates_found', [])
            if dates:
                print(f"  ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –¥–∞—Ç: {len(dates)}")
                print(f"    –ü—Ä–∏–º–µ—Ä—ã: {', '.join(dates[:3])}")

        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")

    def show_statistics_menu(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–µ–∫—Ç–∞"""
        cases = self.assistant.data_manager.load_cases()

        print("\n" + "=" * 60)
        print("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–ï–ö–¢–ê –° NLP ")
        print("=" * 60)

        print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –¥–µ–ª –≤ –±–∞–∑–µ: {len(cases)}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–æ–ª—è–º
        role_stats = {}
        for case in cases:
            role = case.get('user_role', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            role_stats[role] = role_stats.get(role, 0) + 1

        if role_stats:
            print(f"\nüë• –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –†–û–õ–Ø–ú:")
            for role, count in role_stats.items():
                icon = "üë®‚Äç‚öñÔ∏è" if role == "—Å—É–¥—å—è" else "üë®‚Äçüíº" if role == "–∞–¥–≤–æ–∫–∞—Ç" else "üëÆ" if role == "–ø—Ä–æ–∫—É—Ä–æ—Ä" else "‚ùì"
                print(f"  {icon} {role}: {count} –¥–µ–ª")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç—á–µ—Ç–æ–≤
        reports_dir = self.assistant.data_manager.reports_dir
        if os.path.exists(reports_dir):
            reports = [f for f in os.listdir(reports_dir) if f.endswith('.json')]
            print(f"\nüìÑ –û–¢–ß–ï–¢–´:")
            print(f"  ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –æ—Ç—á–µ—Ç–æ–≤: {len(reports)}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ NLP
        print(f"\n NLP-–í–û–ó–ú–û–ñ–ù–û–°–¢–ò:")
        print(f"  ‚Ä¢ –ë–∞–∑–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {len(self.assistant.core.case_examples_db)} –¥–µ–ª")
        print(f"  ‚Ä¢ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(self.assistant.nlp_analyzer.extract_legal_terms(''))}")

        # –ü—Ä–∏–º–µ—Ä NLP-–∞–Ω–∞–ª–∏–∑–∞
        if cases:
            sample_case = cases[0]
            sample_text = sample_case.get('description', '')
            if sample_text:
                nlp_result = self.assistant.analyze_text_nlp(sample_text[:500])
                if "error" not in nlp_result:
                    print(f"\nüìù –ü–†–ò–ú–ï–† NLP-–ê–ù–ê–õ–ò–ó–ê:")
                    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–µ–ª–∞: {nlp_result.get('word_count', 0)} —Å–ª–æ–≤")
                    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {nlp_result.get('complexity_score', 0):.2f}/1.0")

        input("\n‚Üµ –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å...")


def main():
    print("\n" + "=" * 70)
    print("  AI-–ê–°–°–ò–°–¢–ï–ù–¢ –î–õ–Ø –ü–†–ê–í–û–°–£–î–ò–Ø")
    print("  –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞ —Å NLP-–∞–Ω–∞–ª–∏–∑–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª")
    print("=" * 70)

    ui = UserInterface()
    ui.display_main_menu()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ö† –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
